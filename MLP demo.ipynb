{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os, sys, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "num_epochs =100\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_classes, input_size):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=input_size, out_features=1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "        self.linear2 = nn.Linear(in_features=1024, out_features=256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.linear3 = nn.Linear(in_features=256, out_features= num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.linear1(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.bn2(self.linear2(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "class LSTMs(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(LSTMs, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取來源檔案 遮蔽時會讀取以儲存npy檔案\n",
    "new_data=False\n",
    "filter_ths = 10\n",
    "if new_data==True:\n",
    "    raw_data = pd.read_csv('./4602_SARS-CoV-2_pima_0708.csv')\n",
    "else:\n",
    "    # npy path\n",
    "    train_npy = [f'./dataset/X_train_below{filter_ths}.npy', f'./dataset/y_train_below{filter_ths}.npy']\n",
    "    test_npy = [f'./dataset/X_test_below{filter_ths}.npy', f'./dataset/y_test_below{filter_ths}.npy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取新數據集\n",
    "def filter_dataset(data, filter_ths=50):\n",
    "    class_count = data['lineage'].value_counts() #抓csv裡面的class label\n",
    "    class_filter = class_count[class_count[:]>filter_ths] #根據def設定的過濾數值[boolean{total single class nums > ths}]篩選出符合Thresholds的Class\n",
    "    print(class_filter)\n",
    "    print('total data=', sum(class_filter))\n",
    "    class_list = list(dict(class_filter).keys()) #抓出過濾後的class清單名字\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in np.array(data):\n",
    "        for idx, j in enumerate(class_list): #利用enumerate 同時取得Class list的index位置和名稱\n",
    "            if i[0] ==j:\n",
    "                X.append(i[1::].astype(np.float16))\n",
    "                y.append(idx)\n",
    "\n",
    "    # y = F.one_hot(torch.tensor(y), num_classes=len(class_list)) #轉成onehot label -> ['A','B','C'] -> [[1,0,0],[0,1,0],[0,0,1]]\n",
    "    return np.array(X), np.array(y), class_list\n",
    "\n",
    "# 讀取以儲存npy資料集\n",
    "\n",
    "class data_loader:\n",
    "    def __init__(self, x_train, y_train, x_test, y_test):\n",
    "\n",
    "        self.x_train = np.array(x_train)\n",
    "        self.x_test = np.array(x_test)\n",
    "        self.y_train = np.array(y_train)\n",
    "        self.y_test = np.array(y_test)\n",
    "        print(self.x_train.shape, self.y_train.shape, self.x_test.shape, self.y_test.shape)\n",
    "    def get_index(self, data_list, GT_list, b):\n",
    "        labels = []\n",
    "        datas =[]\n",
    "        for patch, i in enumerate(GT_list):\n",
    "            for index, label in enumerate(b):\n",
    "                if i==label:\n",
    "                    labels.append(index)\n",
    "                    datas.append(data_list[patch])\n",
    "        return datas, labels\n",
    "\n",
    "    def npy_loading(self):\n",
    "        # train processing\n",
    "        b1, _, _, w1= np.unique(self.y_train,return_counts=True,return_index=True,return_inverse=True)\n",
    "        # test(valid) processing\n",
    "        b2, _, _, w2= np.unique(self.y_test,return_counts=True,return_index=True,return_inverse=True)\n",
    "        print('train data count')\n",
    "        df1 = pd.DataFrame(w1)\n",
    "        df1.index = b1\n",
    "        print(df1)\n",
    "        print('test data count')\n",
    "        df2 = pd.DataFrame(w2)\n",
    "        df2.index = b2\n",
    "        print(df2)\n",
    "        X_train, Y_train = self.get_index(self.x_train, self.y_train, b2)\n",
    "        X_test, Y_test = self.get_index(self.x_test, self.y_test, b2)\n",
    "\n",
    "        return np.array(X_train), np.array(Y_train), np.array(X_test), np.array(Y_test), b2\n",
    "\n",
    "try:\n",
    "    X, y, class_dict = filter_dataset(raw_data, filter_ths=filter_ths)\n",
    "    np.save(f'Cov-2_f-{filter_ths}_X', X)\n",
    "    np.save(f'Cov-2_f-{filter_ths}_y', y)\n",
    "    X = np.load('./Cov-2_f-50_X.npy')\n",
    "    y = np.load('./Cov-2_f-50_y.npy')\n",
    "    print('X shape:', X.shape)\n",
    "    print('y shape:', y.shape)\n",
    "    class_n = max(y)+1\n",
    "    # zip pytorch tensor dataloader\n",
    "    datas_zip = TensorDataset(torch.tensor(X), torch.tensor(y)) #zip X, y\n",
    "    # # split train and valid dataset\n",
    "    train_size = int((len(datas_zip)*0.75))\n",
    "    test_size = len(datas_zip)-train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(datas_zip, [train_size, test_size])\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "except:\n",
    "    # dataset npy loading\n",
    "    loader = data_loader(np.load(train_npy[0], allow_pickle=True), np.load(train_npy[1], allow_pickle=True), \n",
    "                        np.load(test_npy[0], allow_pickle=True), np.load(test_npy[1], allow_pickle=True))\n",
    "\n",
    "    X_train, y_train,X_test, y_test, test_class = loader.npy_loading()\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "    class_n = len(test_class)\n",
    "    print('train class', max(y_train)+1, 'test class', max(y_test)+1)\n",
    "\n",
    "\n",
    "    # pytorch data zip processing\n",
    "    train_zip = TensorDataset(torch.tensor(X_train), torch.tensor(y_train)) #zip X, y\n",
    "    test_zip = TensorDataset(torch.tensor(X_test), torch.tensor(y_test)) #zip X, y\n",
    "    train_loader = DataLoader(dataset=train_zip, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(dataset=test_zip, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(class_n, input_size=len(train_zip[0][0])).cuda()\n",
    "# summary(model, (1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    classes = torch.argmax(predictions, dim=1)\n",
    "    return torch.mean((classes == labels).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tloss_stack, vloss_stack, t_acc_stack, v_acc_stack = [], [], [], []\n",
    "min_valid_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss = 0.0\n",
    "    batch_train_acc = 0.00\n",
    "    for data, labels in train_loader:\n",
    "        # Forward pass\n",
    "        model.train()\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data.float())\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()*data.size(0)\n",
    "        batch_train_acc += accuracy(output, labels)\n",
    "    tloss_stack.append(loss)\n",
    "    batch_train_acc /= len(train_loader)\n",
    "    t_acc_stack.append(batch_train_acc)\n",
    "\n",
    "    # Validation\n",
    "    valid_loss = 0.0\n",
    "    batch_valid_acc = 0.00\n",
    "    model.eval()\n",
    "    for data, labels in valid_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        output = model(data.float())\n",
    "        loss = criterion(output, labels)\n",
    "        valid_loss = loss.item()*data.size(0)\n",
    "        batch_valid_acc += accuracy(output, labels)\n",
    "    vloss_stack.append(loss)\n",
    "    batch_valid_acc /= len(valid_loader)\n",
    "    v_acc_stack.append(batch_valid_acc)\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "          \n",
    "        # Saving State Dict\n",
    "        torch.save(model, f'saved_model_{filter_ths}.pth')\n",
    "\n",
    "\n",
    "    print(f'Epoch {epoch+1} \\t\\t \\\n",
    "        Train L: {(train_loss / len(train_loader)):.5f} \\t\\t Train Acc: {batch_train_acc:.5f}\\t\\t \\\n",
    "        Valid L: {(valid_loss / len(valid_loader)):.5f} \\t\\t Valid Acc: {batch_valid_acc:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(tloss_stack, label='train loss')\n",
    "plt.plot(vloss_stack, label='valid loss')\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('CE loss',fontsize=18)\n",
    "plt.title('MLP model plot', fontsize=25)\n",
    "plt.legend()\n",
    "plt.savefig(f'./results/{filter_ths}_loss_MLP.jpg')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(t_acc_stack, label='train acc')\n",
    "plt.plot(v_acc_stack, label='valid acc')\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Accuracy',fontsize=18)\n",
    "plt.title('MLP model plot', fontsize=25)\n",
    "plt.legend()\n",
    "plt.savefig(f'./results/{filter_ths}_accuracy_MLP.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "model = torch.load(f'./saved_model_{filter_ths}.pth') \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = [] \n",
    "for data, labels in valid_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        output = model(data.float())\n",
    "        _, preds = torch.max(output, 1) \n",
    "        y_pred.extend(preds.view(-1).detach().cpu().numpy())    \n",
    "        y_true.extend(labels.view(-1).detach().cpu().numpy())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_true, y_pred, normalize='pred') \n",
    "per_cls_acc = cf_matrix.diagonal()/cf_matrix.sum(axis=0)\n",
    "print(per_cls_acc)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "df_cm = pd.DataFrame(cf_matrix, test_class, test_class)\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(df_cm, annot=True, cmap='gist_heat_r')\n",
    "plt.xlabel(\"prediction\", fontsize =15)\n",
    "plt.ylabel(\"label (ground truth)\", fontsize =15)\n",
    "plt.title(f'class num >{filter_ths}', fontsize=22)\n",
    "plt.savefig(f'./results/{filter_ths}_CM.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47ed0dbdec9901086886300defac8fc027ffa2cf340ed530b82da576181f4925"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('lstm_pyt': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}